{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitryPodyachev/bremen/blob/main/3xvideo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/generative-models\n",
        "!apt install mc aria2\n",
        "!export PYTORCH_ALLOC_CONF=expandable_segments:True\n",
        "!pip install einops fairscale fire fsspec invisible-watermark kornia ninja omegaconf open-clip-torch opencv-python pandas pillow pytorch-lightning pyyaml scipy timm tokenizers torch torchaudio torchdata torchmetrics torchvision tqdm transformers triton xformers urllib3 gradio\n",
        "!pip install -q -e generative-models\n",
        "!pip install -q -e git+https://github.com/Stability-AI/datapipelines@main#egg=sdata\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/stable-video-diffusion-img2vid-xt/resolve/main/svd_xt.safetensors?download=true -d /content/checkpoints -o svd_xt.safetensors\n",
        "!mkdir -p /content/scripts/util/detection\n",
        "!ln -s /content/generative-models/scripts/util/detection/p_head_v1.npz /content/scripts/util/detection/p_head_v1.npz\n",
        "!ln -s /content/generative-models/scripts/util/detection/w_head_v1.npz /content/scripts/util/detection/w_head_v1.npz\n"
      ],
      "metadata": {
        "id": "CMfEN7wcuI58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3a14c8-009d-49e7-aae9-8467d5211b5c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'generative-models'...\n",
            "remote: Enumerating objects: 850, done.\u001b[K\n",
            "remote: Counting objects: 100% (361/361), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 850 (delta 286), reused 272 (delta 272), pack-reused 489 (from 1)\u001b[K\n",
            "Receiving objects: 100% (850/850), 42.65 MiB | 25.86 MiB/s, done.\n",
            "Resolving deltas: 100% (437/437), done.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2 mc-data\n",
            "Suggested packages:\n",
            "  arj catdvi | texlive-binaries dbview djvulibre-bin epub-utils genisoimage gv\n",
            "  imagemagick libaspell-dev links | w3m | lynx odt2txt poppler-utils python\n",
            "  python-boto python-tz unar wimtools xpdf | pdf-viewer\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2 mc mc-data\n",
            "0 upgraded, 5 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 3,487 kB of archives.\n",
            "After this operation, 13.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-ares2 amd64 1.18.1-1ubuntu0.22.04.3 [45.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaria2-0 amd64 1.36.0-1 [1,086 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 aria2 amd64 1.36.0-1 [381 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 mc-data all 3:4.8.27-1 [1,427 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 mc amd64 3:4.8.27-1 [547 kB]\n",
            "Fetched 3,487 kB in 1s (3,791 kB/s)\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 121852 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Selecting previously unselected package mc-data.\n",
            "Preparing to unpack .../mc-data_3%3a4.8.27-1_all.deb ...\n",
            "Unpacking mc-data (3:4.8.27-1) ...\n",
            "Selecting previously unselected package mc.\n",
            "Preparing to unpack .../mc_3%3a4.8.27-1_amd64.deb ...\n",
            "Unpacking mc (3:4.8.27-1) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Setting up mc-data (3:4.8.27-1) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up mc (3:4.8.27-1) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.2)\n",
            "Collecting fairscale\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire\n",
            "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (2025.3.0)\n",
            "Collecting invisible-watermark\n",
            "  Downloading invisible_watermark-0.2.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting kornia\n",
            "  Downloading kornia-0.8.2-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting ninja\n",
            "  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (2.3.0)\n",
            "Collecting open-clip-torch\n",
            "  Downloading open_clip_torch-3.2.0-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.13.0.92)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.10.0+cu128)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.10.0+cu128)\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.25.0+cu128)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.6.0)\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.35-py39-none-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (2.5.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from fairscale) (2.0.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire) (3.3.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from invisible-watermark) (1.9.0)\n",
            "Collecting kornia_rs>=0.1.9 (from kornia)\n",
            "  Downloading kornia_rs-0.1.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kornia) (26.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (2025.11.3)\n",
            "Collecting ftfy (from open-clip-torch)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (1.4.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (0.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.15.0)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.15.3-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch) (1.3.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchdata) (2.32.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.24.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.129.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.7)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.22)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.52.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.24.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.41.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.4.2)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch) (1.2.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch) (1.5.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open-clip-torch) (0.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata) (3.4.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.22.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading invisible_watermark-0.2.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia-0.8.2-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "Downloading open_clip_torch-3.2.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.6.1-py3-none-any.whl (857 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.3/857.3 kB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.35-py39-none-manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.3-py3-none-any.whl (31 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332207 sha256=64d421b0d575cc4ac4edab1202da90bb8a6f5624871bf0219ca0cfc504367620\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/88/aa/d84b2cf1bad6b273cbf661640141a82c7b9f496e024f80aac0\n",
            "Successfully built fairscale\n",
            "Installing collected packages: ninja, lightning-utilities, kornia_rs, ftfy, fire, xformers, torchmetrics, kornia, invisible-watermark, fairscale, pytorch-lightning, open-clip-torch\n",
            "Successfully installed fairscale-0.4.13 fire-0.7.1 ftfy-6.3.1 invisible-watermark-0.2.0 kornia-0.8.2 kornia_rs-0.1.10 lightning-utilities-0.15.3 ninja-1.13.0 open-clip-torch-3.2.0 pytorch-lightning-2.6.1 torchmetrics-1.8.2 xformers-0.0.35\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for sgm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "d7a417|\u001b[1;32mOK\u001b[0m  |   164MiB/s|/content/checkpoints/svd_xt.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQFr46OOeYAW",
        "outputId": "da8ff13c-1b5d-46bd-97e9-6426b6844ddc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb 22 22:07:26 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0             28W /   70W |    3205MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A           31088      C   /usr/bin/python3                       3202MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "a6d3f61f",
        "outputId": "c77d4479-60d1-4f48-e25d-96ed4e56c332"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"generative-models\")\n",
        "\n",
        "import os, math, torch, cv2\n",
        "from omegaconf import OmegaConf\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from typing import Optional, List\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.transforms import functional as TF\n",
        "from sgm.util import instantiate_from_config\n",
        "\n",
        "def load_model(config: str, device: str, num_frames: int, num_steps: int):\n",
        "    config = OmegaConf.load(config)\n",
        "    # Set init_device for open_clip_embedding_config to 'cpu' to avoid early GPU allocation\n",
        "    config.model.params.conditioner_config.params.emb_models[0].params.open_clip_embedding_config.params.init_device = \"cpu\"\n",
        "    config.model.params.sampler_config.params.num_steps = num_steps\n",
        "    config.model.params.sampler_config.params.guider_config.params.num_frames = (num_frames)\n",
        "\n",
        "    # Instantiate the model on CPU first\n",
        "    model = instantiate_from_config(config.model).eval().requires_grad_(False)\n",
        "\n",
        "    # Move model.model (the UNet) to device in float16\n",
        "    model.model.to(device=device, dtype=torch.float16)\n",
        "\n",
        "    # Keep conditioner and first_stage_model on CPU initially, as they are moved to CPU later in the sample function\n",
        "    model.conditioner.to(\"cpu\")\n",
        "    model.first_stage_model.to(\"cpu\")\n",
        "\n",
        "    return model\n",
        "\n",
        "num_frames = 16 # Changed from 25 to 16 to fix einops error\n",
        "num_steps = 10\n",
        "model_config = \"generative-models/scripts/sampling/configs/svd_xt.yaml\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Call torch.cuda.empty_cache() before loading the model to ensure maximum available memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = load_model(model_config, device, num_frames, num_steps)\n",
        "\n",
        "# These lines are redundant now as conditioner and first_stage_model are already on CPU\n",
        "# model.conditioner.cpu()\n",
        "# model.first_stage_model.cpu()\n",
        "\n",
        "# This line is handled inside load_model now\n",
        "# model.model.to(dtype=torch.float16)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "model = model.requires_grad_(False)\n",
        "\n",
        "def get_unique_embedder_keys_from_conditioner(conditioner):\n",
        "    return list(set([x.input_key for x in conditioner.embedders]))\n",
        "\n",
        "def get_batch(keys, value_dict, N, T, device, dtype=None):\n",
        "    batch = {}\n",
        "    batch_uc = {}\n",
        "    for key in keys:\n",
        "        if key == \"fps_id\":\n",
        "            batch[key] = (\n",
        "                torch.tensor([value_dict[\"fps_id\"]])\n",
        "                .to(device, dtype=dtype)\n",
        "                .repeat(int(math.prod(N)))\n",
        "            )\n",
        "        elif key == \"motion_bucket_id\":\n",
        "            batch[key] = (\n",
        "                torch.tensor([value_dict[\"motion_bucket_id\"]])\n",
        "                .to(device, dtype=dtype)\n",
        "                .repeat(int(math.prod(N)))\n",
        "            )\n",
        "        elif key == \"cond_aug\":\n",
        "            batch[key] = repeat(\n",
        "                torch.tensor([value_dict[\"cond_aug\"]]).to(device, dtype=dtype),\n",
        "                \"1 -> b\",\n",
        "                b=math.prod(N),\n",
        "            )\n",
        "        elif key == \"cond_frames\":\n",
        "            batch[key] = repeat(value_dict[\"cond_frames\"], \"1 ... -> b ...\", b=N[0])\n",
        "        elif key == \"cond_frames_without_noise\":\n",
        "            batch[key] = repeat(\n",
        "                value_dict[\"cond_frames_without_noise\"], \"1 ... -> b ...\", b=N[0]\n",
        "            )\n",
        "        else:\n",
        "            batch[key] = value_dict[key]\n",
        "    if T is not None:\n",
        "        batch[\"num_video_frames\"] = T\n",
        "    for key in batch.keys():\n",
        "        if key not in batch_uc and isinstance(batch[key], torch.Tensor):\n",
        "            batch_uc[key] = torch.clone(batch[key])\n",
        "    return batch, batch_uc\n",
        "\n",
        "def sample(\n",
        "    input_path: str = \"/content/test_image.png\",\n",
        "    resize_image: bool = False,\n",
        "    num_frames: Optional[int] = None,\n",
        "    num_steps: Optional[int] = None,\n",
        "    fps_id: int = 6,\n",
        "    motion_bucket_id: int = 127,\n",
        "    cond_aug: float = 0.02,\n",
        "    seed: int = 23,\n",
        "    decoding_t: int = 14,  # Number of frames decoded at a time! This eats most VRAM. Reduce if necessary.\n",
        "    device: str = \"cuda\",\n",
        "    output_folder: Optional[str] = \"/content/outputs\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Simple script to generate a single sample conditioned on an image `input_path` or multiple images, one for each\n",
        "    image file in folder `input_path`.\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    path = Path(input_path)\n",
        "    all_img_paths = []\n",
        "    # Check if input_path is a URL\n",
        "    if input_path.startswith(\"http://\") or input_path.startswith(\"https://\"):\n",
        "        import requests\n",
        "        from io import BytesIO\n",
        "        response = requests.get(input_path)\n",
        "        image = Image.open(BytesIO(response.content))\n",
        "        # Save the image temporarily if the sample function expects a filepath, or pass the PIL Image object\n",
        "        # For now, let's assume `Image.open` can handle BytesIO and subsequent processing handles PIL Image.\n",
        "        # If it needs a path, we'd save it to a temp file here.\n",
        "        # The current `sample` function's internal logic expects input_path to be a path to open, so we need to save it.\n",
        "        temp_image_path = \"/tmp/temp_image.png\"\n",
        "        image.save(temp_image_path)\n",
        "        all_img_paths = [temp_image_path]\n",
        "    elif path.is_file():\n",
        "        if any([input_path.endswith(x) for x in [\"jpg\", \"jpeg\", \"png\"]]):\n",
        "            all_img_paths = [input_path]\n",
        "        else:\n",
        "            raise ValueError(\"Path is not valid image file.\")\n",
        "    elif path.is_dir():\n",
        "        all_img_paths = sorted(\n",
        "            [\n",
        "                f\n",
        "                for f in path.iterdir()\n",
        "                if f.is_file() and f.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n",
        "            ]\n",
        "        )\n",
        "        if len(all_img_paths) == 0:\n",
        "            raise ValueError(\"Folder does not contain any images.\")\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    all_out_paths = []\n",
        "    for input_img_path in all_img_paths:\n",
        "        with Image.open(input_img_path) as image:\n",
        "            if image.mode == \"RGBA\":\n",
        "                image = image.convert(\"RGB\")\n",
        "            if resize_image and image.size != (1024, 576):\n",
        "                print(f\"Resizing {image.size} to (1024, 576)\")\n",
        "                image = TF.resize(TF.resize(image, 1024), (576, 1024))\n",
        "            w, h = image.size\n",
        "            if h % 64 != 0 or w % 64 != 0:\n",
        "                width, height = map(lambda x: x - x % 64, (w, h))\n",
        "                image = image.resize((width, height))\n",
        "                print(\n",
        "                    f\"WARNING: Your image is of size {h}x{w} which is not divisible by 64. We are resizing to {height}x{width}!\"\n",
        "                )\n",
        "            image = ToTensor()(image)\n",
        "            image = image * 2.0 - 1.0\n",
        "\n",
        "        image = image.unsqueeze(0).to(device)\n",
        "        H, W = image.shape[2:]\n",
        "        assert image.shape[1] == 3\n",
        "        F = 8\n",
        "        C = 4\n",
        "        shape = (num_frames, C, H // F, W // F)\n",
        "        if (H, W) != (576, 1024):\n",
        "            print(\n",
        "                \"WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\"\n",
        "            )\n",
        "        if motion_bucket_id > 255:\n",
        "            print(\n",
        "                \"WARNING: High motion bucket! This may lead to suboptimal performance.\"\n",
        "            )\n",
        "        if fps_id < 5:\n",
        "            print(\"WARNING: Small fps value! This may lead to suboptimal performance.\")\n",
        "        if fps_id > 30:\n",
        "            print(\"WARNING: Large fps value! This may lead to suboptimal performance.\")\n",
        "\n",
        "        value_dict = {}\n",
        "        value_dict[\"motion_bucket_id\"] = motion_bucket_id\n",
        "        value_dict[\"fps_id\"] = fps_id\n",
        "        value_dict[\"cond_aug\"] = cond_aug\n",
        "        value_dict[\"cond_frames_without_noise\"] = image\n",
        "        value_dict[\"cond_frames\"] = image + cond_aug * torch.randn_like(image)\n",
        "        value_dict[\"cond_aug\"] = cond_aug\n",
        "        # low vram mode\n",
        "        model.conditioner.cpu()\n",
        "        model.first_stage_model.cpu()\n",
        "        torch.cuda.empty_cache()\n",
        "        model.sampler.verbose = True\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with torch.autocast(device):\n",
        "                model.conditioner.to(device)\n",
        "                batch, batch_uc = get_batch(\n",
        "                    get_unique_embedder_keys_from_conditioner(model.conditioner),\n",
        "                    value_dict,\n",
        "                    [1, num_frames],\n",
        "                    T=num_frames,\n",
        "                    device=device,\n",
        "                )\n",
        "                c, uc = model.conditioner.get_unconditional_conditioning(\n",
        "                    batch,\n",
        "                    batch_uc=batch_uc,\n",
        "                    force_uc_zero_embeddings=[\n",
        "                        \"cond_frames\",\n",
        "                        \"cond_frames_without_noise\",\n",
        "                    ],\n",
        "                )\n",
        "                model.conditioner.cpu()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                # from here, dtype is fp16\n",
        "                for k in [\"crossattn\", \"concat\"]:\n",
        "                    uc[k] = repeat(uc[k], \"b ... -> b t ...\", t=num_frames)\n",
        "                    uc[k] = rearrange(uc[k], \"b t ... -> (b t) ...\", t=num_frames)\n",
        "                    c[k] = repeat(c[k], \"b ... -> b t ...\", t=num_frames)\n",
        "                    c[k] = rearrange(c[k], \"b t ... -> (b t) ...\", t=num_frames)\n",
        "                for k in uc.keys():\n",
        "                    uc[k] = uc[k].to(dtype=torch.float16)\n",
        "                    c[k] = c[k].to(dtype=torch.float16)\n",
        "\n",
        "                randn = torch.randn(shape, device=device, dtype=torch.float16)\n",
        "                additional_model_inputs = {}\n",
        "                additional_model_inputs[\"image_only_indicator\"] = torch.zeros(2, num_frames).to(device)\n",
        "                additional_model_inputs[\"num_video_frames\"] = batch[\"num_video_frames\"]\n",
        "\n",
        "                for k in additional_model_inputs:\n",
        "                    if isinstance(additional_model_inputs[k], torch.Tensor):\n",
        "                        additional_model_inputs[k] = additional_model_inputs[k].to(dtype=torch.float16)\n",
        "\n",
        "                def denoiser(input, sigma, c):\n",
        "                    return model.denoiser(model.model, input, sigma, c, **additional_model_inputs)\n",
        "\n",
        "                samples_z = model.sampler(denoiser, randn, cond=c, uc=uc)\n",
        "                samples_z.to(dtype=model.first_stage_model.dtype)\n",
        "                model.en_and_decode_n_samples_a_time = decoding_t\n",
        "                model.first_stage_model.to(device)\n",
        "                samples_x = model.decode_first_stage(samples_z)\n",
        "                samples = torch.clamp((samples_x + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                model.first_stage_model.cpu()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                os.makedirs(output_folder, exist_ok=True)\n",
        "                base_count = len(glob(os.path.join(output_folder, \"*.mp4\")))\n",
        "                video_path = os.path.join(output_folder, f\"{base_count:06d}.mp4\")\n",
        "                writer = cv2.VideoWriter(\n",
        "                    video_path,\n",
        "                    cv2.VideoWriter_fourcc(*\"MP4V\"),\n",
        "                    fps_id + 1,\n",
        "                    (samples.shape[-1], samples.shape[-2]),\n",
        "                )\n",
        "                vid = (\n",
        "                    (rearrange(samples, \"t c h w -> t h w c\") * 255)\n",
        "                    .cpu()\n",
        "                    .numpy()\n",
        "                    .astype(np.uint8)\n",
        "                )\n",
        "                for frame in vid:\n",
        "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                    writer.write(frame)\n",
        "                writer.release()\n",
        "                all_out_paths.append(video_path)\n",
        "    return all_out_paths\n",
        "\n",
        "import gradio as gr\n",
        "import random\n",
        "\n",
        "def infer(input_path: str, resize_image: bool, n_steps: int, decoding_t: int) -> List[str]:\n",
        "  generated_videos = []\n",
        "  for i in range(2): # Generate 2 videos\n",
        "    seed = random.randint(0, 2**32) # Use a new random seed for each generation\n",
        "    print(f\"Generating video {i+1} with seed: {seed}\")\n",
        "    output_paths = sample(\n",
        "      input_path=input_path,\n",
        "      resize_image=resize_image,\n",
        "      num_frames=num_frames, # Use global num_frames\n",
        "      num_steps=n_steps,\n",
        "      fps_id=6,\n",
        "      motion_bucket_id=127,\n",
        "      cond_aug=0.02,\n",
        "      seed=seed,\n",
        "      decoding_t=decoding_t,  # Number of frames decoded at a time! This eats most VRAM. Reduce if necessary.\n",
        "      device=device,\n",
        "    )\n",
        "    generated_videos.append(output_paths[0])\n",
        "  return generated_videos\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  with gr.Column():\n",
        "    image = gr.Image(label=\"input image\", type=\"filepath\")\n",
        "    resize_image = gr.Checkbox(label=\"resize to optimal size\", value=True)\n",
        "    btn = gr.Button(\"Run\")\n",
        "    with gr.Accordion(label=\"Advanced options\", open=False):\n",
        "      # Removed n_frames input as it cannot be changed dynamically\n",
        "      n_steps = gr.Number(precision=0, label=\"number of steps\", value=num_steps)\n",
        "      # Removed seed input as it's now handled internally for multiple generations\n",
        "      decoding_t = gr.Number(precision=0, label=\"number of frames decoded at a time\", value=2)\n",
        "  with gr.Column():\n",
        "    gr.Markdown(\"### Generated Videos:\")\n",
        "    video_out1 = gr.Video(label=\"Generated Video 1\")\n",
        "    video_out2 = gr.Video(label=\"Generated Video 2\")\n",
        "\n",
        "  examples = [[\"https://user-images.githubusercontent.com/33302880/284758167-367a25d8-8d7b-42d3-8391-6d82813c7b0f.png\"]]\n",
        "  inputs = [image, resize_image, n_steps, decoding_t]\n",
        "  outputs = [video_out1, video_out2]\n",
        "  btn.click(infer, inputs=inputs, outputs=outputs)\n",
        "  gr.Examples(examples=examples, inputs=inputs, outputs=outputs, fn=infer)\n",
        "  demo.queue().launch(debug=True, share=True, inline=False, show_error=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "Initialized embedder #0: FrozenOpenCLIPImagePredictionEmbedder with 683800065 params. Trainable: False\n",
            "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
            "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
            "Initialized embedder #3: VideoPredictionEmbedderWithEncoder with 83653863 params. Trainable: False\n",
            "Initialized embedder #4: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
            "Restored from checkpoints/svd_xt.safetensors with 0 missing and 0 unexpected keys\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'video_out3' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1933826978.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    310\u001b[0m   \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"https://user-images.githubusercontent.com/33302880/284758167-367a25d8-8d7b-42d3-8391-6d82813c7b0f.png\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoding_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvideo_out1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_out2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_out3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m   \u001b[0mbtn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m   \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'video_out3' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b88e5fa"
      },
      "source": [
        "Чтобы очистить память GPU от всех загруженных моделей и тензоров, нужно явно удалить переменные, которые на них ссылаются, а затем вызвать `torch.cuda.empty_cache()`.\n",
        "\n",
        "Следующий код поможет вам это сделать. Он ищет в глобальном пространстве имен все объекты, которые являются экземплярами `torch.nn.Module` (т.е. ваши модели) или `torch.Tensor`, и если они находятся на CUDA, удаляет их.\n",
        "\n",
        "**Важно:** Убедитесь, что вы действительно больше не нуждаетесь в этих моделях/тензорах, прежде чем запускать этот код, так как он безвозвратно удалит их из памяти."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a94cb5cf",
        "outputId": "aa3bd127-5ae9-47af-b8f5-5d92f71554c8"
      },
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "def clear_all_gpu_memory():\n",
        "    print(\"Attempting to clear all GPU memory...\")\n",
        "    # Удалить все ссылки на модели и тензоры из глобального пространства имен\n",
        "    for name in dir():\n",
        "        if not name.startswith('_'): # Избегаем внутренних переменных Python\n",
        "            var = globals()[name]\n",
        "            if isinstance(var, (torch.nn.Module, torch.Tensor)):\n",
        "                if hasattr(var, 'is_cuda') and var.is_cuda:\n",
        "                    print(f\"Deleting GPU object: {name} (Type: {type(var)}) \")\n",
        "                    del globals()[name]\n",
        "            elif isinstance(var, (list, tuple, dict)):\n",
        "                # Для коллекций, попытаться очистить их содержимое, если оно на GPU\n",
        "                # Это упрощенный подход, для глубокой очистки нужен рекурсивный обход\n",
        "                pass\n",
        "\n",
        "    # Запустить сборщик мусора Python\n",
        "    gc.collect()\n",
        "    # Очистить кэш памяти GPU\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU memory cleared. Free memory: \", torch.cuda.memory_reserved() / 1024**3, \"GB\")\n",
        "\n",
        "# Вызов функции для очистки памяти\n",
        "clear_all_gpu_memory()\n",
        "\n",
        "# Пример: если 'model' была определена как глобальная переменная, она будет удалена.\n",
        "# Если у вас есть другие глобальные переменные, такие как 'samples_z', 'c', 'uc', 'randn' из функции sample,\n",
        "# вы можете явно удалить и их:\n",
        "# del samples_z\n",
        "# del c\n",
        "# del uc\n",
        "# del randn\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# Если вы хотите очистить модель 'model' из предыдущей ячейки:\n",
        "if 'model' in globals():\n",
        "    print(\"Deleting global 'model' variable.\")\n",
        "    del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to clear all GPU memory...\n",
            "GPU memory cleared. Free memory:  2.99609375 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VjYy0F2gZIPR",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "009e2a8c-c8e9-4f32-f835-d37c2ce08c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-826662369.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# These lines are redundant now as conditioner and first_stage_model are already on CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-826662369.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(config, device, num_frames, num_steps)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Instantiate the model on CPU first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstantiate_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Move model.model (the UNet) to device in float16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/generative-models/sgm/util.py\u001b[0m in \u001b[0;36minstantiate_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected key `target` to instantiate.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_obj_from_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/generative-models/sgm/models/diffusion.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, network_config, denoiser_config, first_stage_config, conditioner_config, sampler_config, optimizer_config, scheduler_config, loss_fn_config, network_wrapper, ckpt_path, use_ema, ema_decay_rate, scale_factor, disable_first_stage_autocast, input_key, log_keys, no_cond_log, compile_model, en_and_decode_n_samples_a_time)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"torch.optim.AdamW\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         )\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstantiate_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         self.model = get_obj_from_str(default(network_wrapper, OPENAIUNETWRAPPER))(\n\u001b[1;32m     50\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/generative-models/sgm/util.py\u001b[0m in \u001b[0;36minstantiate_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected key `target` to instantiate.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_obj_from_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/generative-models/sgm/modules/diffusionmodules/video_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, model_channels, out_channels, num_res_blocks, attention_resolutions, dropout, channel_mult, conv_resample, dims, num_classes, use_checkpoint, num_heads, num_head_channels, num_heads_upsample, use_scale_shift_norm, resblock_updown, transformer_depth, transformer_depth_middle, context_dim, time_downup, time_context_dim, extra_ff_mix_layer, use_spatial_context, merge_strategy, merge_factor, spatial_transformer_attn_type, video_kernel_size, use_linear_in_transformer, adm_in_channels, disable_temporal_crossattention, max_ddpm_temb_period)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                     layers.append(\n\u001b[0;32m--> 281\u001b[0;31m                         get_attention_layer(\n\u001b[0m\u001b[1;32m    282\u001b[0m                             \u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                             \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/generative-models/sgm/modules/diffusionmodules/video_model.py\u001b[0m in \u001b[0;36mget_attention_layer\u001b[0;34m(ch, num_heads, dim_head, depth, context_dim, use_checkpoint, disabled_sa)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mdisabled_sa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         ):\n\u001b[0;32m--> 207\u001b[0;31m             return SpatialVideoTransformer(\n\u001b[0m\u001b[1;32m    208\u001b[0m                 \u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/generative-models/sgm/modules/video_attention.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, n_heads, d_head, depth, dropout, use_linear, context_dim, use_spatial_context, timesteps, merge_strategy, merge_factor, time_context_dim, ff_in, checkpoint, time_depth, attn_mode, disable_self_attn, disable_temporal_crossattention, max_time_embed_period)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mmax_time_embed_period\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     ):\n\u001b[0;32m--> 169\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/generative-models/sgm/modules/attention.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, n_heads, d_head, depth, dropout, context_dim, disable_self_attn, use_linear, attn_type, use_checkpoint, sdp_backend)\u001b[0m\n\u001b[1;32m    673\u001b[0m             )\n\u001b[1;32m    674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         self.transformer_blocks = nn.ModuleList(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# https://github.com/pytorch/pytorch/issues/57109\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;31m# Calculate uniform bounds from standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"generative-models\")\n",
        "\n",
        "import os, math, torch, cv2\n",
        "from omegaconf import OmegaConf\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from typing import Optional, List\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.transforms import functional as TF\n",
        "from sgm.util import instantiate_from_config\n",
        "\n",
        "def load_model(config: str, device: str, num_frames: int, num_steps: int):\n",
        "    config = OmegaConf.load(config)\n",
        "    # Set init_device for open_clip_embedding_config to 'cpu' to avoid early GPU allocation\n",
        "    config.model.params.conditioner_config.params.emb_models[0].params.open_clip_embedding_config.params.init_device = \"cpu\"\n",
        "    config.model.params.sampler_config.params.num_steps = num_steps\n",
        "    config.model.params.sampler_config.params.guider_config.params.num_frames = (num_frames)\n",
        "\n",
        "    # Instantiate the model on CPU first\n",
        "    model = instantiate_from_config(config.model).eval().requires_grad_(False)\n",
        "\n",
        "    # Move model.model (the UNet) to device in float16\n",
        "    model.model.to(device=device, dtype=torch.float16)\n",
        "\n",
        "    # Keep conditioner and first_stage_model on CPU initially, as they are moved to CPU later in the sample function\n",
        "    model.conditioner.to(\"cpu\")\n",
        "    model.first_stage_model.to(\"cpu\")\n",
        "\n",
        "    return model\n",
        "\n",
        "num_frames = 48 # Changed from 25 to 16 to fix einops error\n",
        "num_steps = 30\n",
        "model_config = \"generative-models/scripts/sampling/configs/svd_xt.yaml\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Call torch.cuda.empty_cache() before loading the model to ensure maximum available memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = load_model(model_config, device, num_frames, num_steps)\n",
        "\n",
        "# These lines are redundant now as conditioner and first_stage_model are already on CPU\n",
        "# model.conditioner.cpu()\n",
        "# model.first_stage_model.cpu()\n",
        "\n",
        "# This line is handled inside load_model now\n",
        "# model.model.to(dtype=torch.float16)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "model = model.requires_grad_(False)\n",
        "\n",
        "def get_unique_embedder_keys_from_conditioner(conditioner):\n",
        "    return list(set([x.input_key for x in conditioner.embedders]))\n",
        "\n",
        "def get_batch(keys, value_dict, N, T, device, dtype=None):\n",
        "    batch = {}\n",
        "    batch_uc = {}\n",
        "    for key in keys:\n",
        "        if key == \"fps_id\":\n",
        "            batch[key] = (\n",
        "                torch.tensor([value_dict[\"fps_id\"]])\n",
        "                .to(device, dtype=dtype)\n",
        "                .repeat(int(math.prod(N)))\n",
        "            )\n",
        "        elif key == \"motion_bucket_id\":\n",
        "            batch[key] = (\n",
        "                torch.tensor([value_dict[\"motion_bucket_id\"]])\n",
        "                .to(device, dtype=dtype)\n",
        "                .repeat(int(math.prod(N)))\n",
        "            )\n",
        "        elif key == \"cond_aug\":\n",
        "            batch[key] = repeat(\n",
        "                torch.tensor([value_dict[\"cond_aug\"]]).to(device, dtype=dtype),\n",
        "                \"1 -> b\",\n",
        "                b=math.prod(N),\n",
        "            )\n",
        "        elif key == \"cond_frames\":\n",
        "            batch[key] = repeat(value_dict[\"cond_frames\"], \"1 ... -> b ...\", b=N[0])\n",
        "        elif key == \"cond_frames_without_noise\":\n",
        "            batch[key] = repeat(\n",
        "                value_dict[\"cond_frames_without_noise\"], \"1 ... -> b ...\", b=N[0]\n",
        "            )\n",
        "        else:\n",
        "            batch[key] = value_dict[key]\n",
        "    if T is not None:\n",
        "        batch[\"num_video_frames\"] = T\n",
        "    for key in batch.keys():\n",
        "        if key not in batch_uc and isinstance(batch[key], torch.Tensor):\n",
        "            batch_uc[key] = torch.clone(batch[key])\n",
        "    return batch, batch_uc\n",
        "\n",
        "def sample(\n",
        "    input_path: str = \"/content/test_image.png\",\n",
        "    resize_image: bool = False,\n",
        "    num_frames: Optional[int] = None,\n",
        "    num_steps: Optional[int] = None,\n",
        "    fps_id: int = 6,\n",
        "    motion_bucket_id: int = 127,\n",
        "    cond_aug: float = 0.02,\n",
        "    seed: int = 23,\n",
        "    decoding_t: int = 14,  # Number of frames decoded at a time! This eats most VRAM. Reduce if necessary.\n",
        "    device: str = \"cuda\",\n",
        "    output_folder: Optional[str] = \"/content/outputs\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Simple script to generate a single sample conditioned on an image `input_path` or multiple images, one for each\n",
        "    image file in folder `input_path`.\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    path = Path(input_path)\n",
        "    all_img_paths = []\n",
        "    # Check if input_path is a URL\n",
        "    if input_path.startswith(\"http://\") or input_path.startswith(\"https://\"):\n",
        "        import requests\n",
        "        from io import BytesIO\n",
        "        response = requests.get(input_path)\n",
        "        image = Image.open(BytesIO(response.content))\n",
        "        # Save the image temporarily if the sample function expects a filepath, or pass the PIL Image object\n",
        "        # For now, let's assume `Image.open` can handle BytesIO and subsequent processing handles PIL Image.\n",
        "        # If it needs a path, we'd save it to a temp file here.\n",
        "        # The current `sample` function's internal logic expects input_path to be a path to open, so we need to save it.\n",
        "        temp_image_path = \"/tmp/temp_image.png\"\n",
        "        image.save(temp_image_path)\n",
        "        all_img_paths = [temp_image_path]\n",
        "    elif path.is_file():\n",
        "        if any([input_path.endswith(x) for x in [\"jpg\", \"jpeg\", \"png\"]]):\n",
        "            all_img_paths = [input_path]\n",
        "        else:\n",
        "            raise ValueError(\"Path is not valid image file.\")\n",
        "    elif path.is_dir():\n",
        "        all_img_paths = sorted(\n",
        "            [\n",
        "                f\n",
        "                for f in path.iterdir()\n",
        "                if f.is_file() and f.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n",
        "            ]\n",
        "        )\n",
        "        if len(all_img_paths) == 0:\n",
        "            raise ValueError(\"Folder does not contain any images.\")\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    all_out_paths = []\n",
        "    for input_img_path in all_img_paths:\n",
        "        with Image.open(input_img_path) as image:\n",
        "            if image.mode == \"RGBA\":\n",
        "                image = image.convert(\"RGB\")\n",
        "            if resize_image and image.size != (1024, 576):\n",
        "                print(f\"Resizing {image.size} to (1024, 576)\")\n",
        "                image = TF.resize(TF.resize(image, 1024), (576, 1024))\n",
        "            w, h = image.size\n",
        "            if h % 64 != 0 or w % 64 != 0:\n",
        "                width, height = map(lambda x: x - x % 64, (w, h))\n",
        "                image = image.resize((width, height))\n",
        "                print(\n",
        "                    f\"WARNING: Your image is of size {h}x{w} which is not divisible by 64. We are resizing to {height}x{width}!\"\n",
        "                )\n",
        "            image = ToTensor()(image)\n",
        "            image = image * 2.0 - 1.0\n",
        "\n",
        "        image = image.unsqueeze(0).to(device)\n",
        "        H, W = image.shape[2:]\n",
        "        assert image.shape[1] == 3\n",
        "        F = 8\n",
        "        C = 4\n",
        "        shape = (num_frames, C, H // F, W // F)\n",
        "        if (H, W) != (576, 1024):\n",
        "            print(\n",
        "                \"WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\"\n",
        "            )\n",
        "        if motion_bucket_id > 255:\n",
        "            print(\n",
        "                \"WARNING: High motion bucket! This may lead to suboptimal performance.\"\n",
        "            )\n",
        "        if fps_id < 5:\n",
        "            print(\"WARNING: Small fps value! This may lead to suboptimal performance.\")\n",
        "        if fps_id > 30:\n",
        "            print(\"WARNING: Large fps value! This may lead to suboptimal performance.\")\n",
        "\n",
        "        value_dict = {}\n",
        "        value_dict[\"motion_bucket_id\"] = motion_bucket_id\n",
        "        value_dict[\"fps_id\"] = fps_id\n",
        "        value_dict[\"cond_aug\"] = cond_aug\n",
        "        value_dict[\"cond_frames_without_noise\"] = image\n",
        "        value_dict[\"cond_frames\"] = image + cond_aug * torch.randn_like(image)\n",
        "        value_dict[\"cond_aug\"] = cond_aug\n",
        "        # low vram mode\n",
        "        model.conditioner.cpu()\n",
        "        model.first_stage_model.cpu()\n",
        "        torch.cuda.empty_cache()\n",
        "        model.sampler.verbose = True\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with torch.autocast(device):\n",
        "                model.conditioner.to(device)\n",
        "                batch, batch_uc = get_batch(\n",
        "                    get_unique_embedder_keys_from_conditioner(model.conditioner),\n",
        "                    value_dict,\n",
        "                    [1, num_frames],\n",
        "                    T=num_frames,\n",
        "                    device=device,\n",
        "                )\n",
        "                c, uc = model.conditioner.get_unconditional_conditioning(\n",
        "                    batch,\n",
        "                    batch_uc=batch_uc,\n",
        "                    force_uc_zero_embeddings=[\n",
        "                        \"cond_frames\",\n",
        "                        \"cond_frames_without_noise\",\n",
        "                    ],\n",
        "                )\n",
        "                model.conditioner.cpu()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                # from here, dtype is fp16\n",
        "                for k in [\"crossattn\", \"concat\"]:\n",
        "                    uc[k] = repeat(uc[k], \"b ... -> b t ...\", t=num_frames)\n",
        "                    uc[k] = rearrange(uc[k], \"b t ... -> (b t) ...\", t=num_frames)\n",
        "                    c[k] = repeat(c[k], \"b ... -> b t ...\", t=num_frames)\n",
        "                    c[k] = rearrange(c[k], \"b t ... -> (b t) ...\", t=num_frames)\n",
        "                for k in uc.keys():\n",
        "                    uc[k] = uc[k].to(dtype=torch.float16)\n",
        "                    c[k] = c[k].to(dtype=torch.float16)\n",
        "\n",
        "                randn = torch.randn(shape, device=device, dtype=torch.float16)\n",
        "                additional_model_inputs = {}\n",
        "                additional_model_inputs[\"image_only_indicator\"] = torch.zeros(2, num_frames).to(device)\n",
        "                additional_model_inputs[\"num_video_frames\"] = batch[\"num_video_frames\"]\n",
        "\n",
        "                for k in additional_model_inputs:\n",
        "                    if isinstance(additional_model_inputs[k], torch.Tensor):\n",
        "                        additional_model_inputs[k] = additional_model_inputs[k].to(dtype=torch.float16)\n",
        "\n",
        "                def denoiser(input, sigma, c):\n",
        "                    return model.denoiser(model.model, input, sigma, c, **additional_model_inputs)\n",
        "\n",
        "                samples_z = model.sampler(denoiser, randn, cond=c, uc=uc)\n",
        "                samples_z.to(dtype=model.first_stage_model.dtype)\n",
        "                model.en_and_decode_n_samples_a_time = decoding_t\n",
        "                model.first_stage_model.to(device)\n",
        "                samples_x = model.decode_first_stage(samples_z)\n",
        "                samples = torch.clamp((samples_x + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                model.first_stage_model.cpu()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                os.makedirs(output_folder, exist_ok=True)\n",
        "                base_count = len(glob(os.path.join(output_folder, \"*.mp4\")))\n",
        "                video_path = os.path.join(output_folder, f\"{base_count:06d}.mp4\")\n",
        "                writer = cv2.VideoWriter(\n",
        "                    video_path,\n",
        "                    cv2.VideoWriter_fourcc(*\"MP4V\"),\n",
        "                    fps_id + 1,\n",
        "                    (samples.shape[-1], samples.shape[-2]),\n",
        "                )\n",
        "                vid = (\n",
        "                    (rearrange(samples, \"t c h w -> t h w c\") * 255)\n",
        "                    .cpu()\n",
        "                    .numpy()\n",
        "                    .astype(np.uint8)\n",
        "                )\n",
        "                for frame in vid:\n",
        "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                    writer.write(frame)\n",
        "                writer.release()\n",
        "                all_out_paths.append(video_path)\n",
        "    return all_out_paths\n",
        "\n",
        "import gradio as gr\n",
        "import random\n",
        "\n",
        "def infer(input_path: str, resize_image: bool, n_steps: int, decoding_t: int) -> List[str]:\n",
        "  generated_videos = []\n",
        "  for i in range(3): # Generate 3 videos\n",
        "    seed = random.randint(0, 2**32) # Use a new random seed for each generation\n",
        "    print(f\"Generating video {i+1} with seed: {seed}\")\n",
        "    output_paths = sample(\n",
        "      input_path=input_path,\n",
        "      resize_image=resize_image,\n",
        "      num_frames=num_frames, # Use global num_frames\n",
        "      num_steps=n_steps,\n",
        "      fps_id=6,\n",
        "      motion_bucket_id=127,\n",
        "      cond_aug=0.02,\n",
        "      seed=seed,\n",
        "      decoding_t=decoding_t,  # Number of frames decoded at a time! This eats most VRAM. Reduce if necessary.\n",
        "      device=device,\n",
        "    )\n",
        "    generated_videos.append(output_paths[0])\n",
        "  return generated_videos\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  with gr.Column():\n",
        "    image = gr.Image(label=\"input image\", type=\"filepath\")\n",
        "    resize_image = gr.Checkbox(label=\"resize to optimal size\", value=True)\n",
        "    btn = gr.Button(\"Run\")\n",
        "    with gr.Accordion(label=\"Advanced options\", open=False):\n",
        "      # Removed n_frames input as it cannot be changed dynamically\n",
        "      n_steps = gr.Number(precision=0, label=\"number of steps\", value=num_steps)\n",
        "      # Removed seed input as it's now handled internally for multiple generations\n",
        "      decoding_t = gr.Number(precision=0, label=\"number of frames decoded at a time\", value=2)\n",
        "  with gr.Column():\n",
        "    gr.Markdown(\"### Generated Videos:\")\n",
        "    video_out1 = gr.Video(label=\"Generated Video 1\")\n",
        "    video_out2 = gr.Video(label=\"Generated Video 2\")\n",
        "\n",
        "\n",
        "  examples = [[\"https://user-images.githubusercontent.com/33302880/284758167-367a25d8-8d7b-42d3-8391-6d82813c7b0f.png\"]]\n",
        "  inputs = [image, resize_image, n_steps, decoding_t]\n",
        "  outputs = [video_out1, video_out2]\n",
        "  btn.click(infer, inputs=inputs, outputs=outputs)\n",
        "  gr.Examples(examples=examples, inputs=inputs, outputs=outputs, fn=infer)\n",
        "  demo.queue().launch(debug=True, share=True, inline=False, show_error=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}